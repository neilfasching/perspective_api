{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "# Define the retry decorator with exponential backoff\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def analyze_perspective(text, api_key):\n",
    "    url = \"https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze\"\n",
    "    data = {\n",
    "        'comment': {'text': text},\n",
    "        'languages': ['en'],\n",
    "        'requestedAttributes': {\n",
    "            'TOXICITY': {}, 'SEVERE_TOXICITY': {}, 'INSULT': {}, 'PROFANITY': {}, \n",
    "            'THREAT': {}, 'IDENTITY_ATTACK': {}\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, params={'key': api_key}, json=data)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        scores = {attr: result['attributeScores'][attr]['summaryScore']['value'] for attr in result['attributeScores']}\n",
    "        return scores\n",
    "    \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 429:  # Rate limit exceeded\n",
    "            logging.warning(f\"Rate limit exceeded. Retrying... Error: {str(e)}\")\n",
    "            raise  # This will trigger the retry\n",
    "        else:\n",
    "            logging.error(f\"HTTP error: {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calling Perspective API: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def check_toxicity(data, api_key, phrase = 'final_phrase'):\n",
    "    new_columns = [\n",
    "        'toxicity', 'severe_toxicity', 'insult', 'profanity', \n",
    "        'threat', 'identity_attack'\n",
    "    ]\n",
    "    \n",
    "    for col in new_columns:\n",
    "        if col not in data.columns:\n",
    "            data[col] = None\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        if index % 500 == 0 and index != 0:\n",
    "            data.to_parquet('backup.pq')\n",
    "            logging.info(f\"Processed {index} rows. Backup saved.\")\n",
    "        \n",
    "        text = row[phrase]\n",
    "        \n",
    "        try:\n",
    "            results = analyze_perspective(text, api_key)\n",
    "            if results:\n",
    "                data.at[index, 'toxicity'] = results.get('TOXICITY')\n",
    "                data.at[index, 'severe_toxicity'] = results.get('SEVERE_TOXICITY')\n",
    "                data.at[index, 'insult'] = results.get('INSULT')\n",
    "                data.at[index, 'profanity'] = results.get('PROFANITY')\n",
    "                data.at[index, 'threat'] = results.get('THREAT')\n",
    "                data.at[index, 'identity_attack'] = results.get('IDENTITY_ATTACK')\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing row {index}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    data.to_parquet('final_backup.pq')\n",
    "    logging.info(\"Processing completed. Final backup saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = \"xxx\"  # Replace with your actual Perspective API key\n",
    "\n",
    "data = pd.DataFrame({'final_phrase': [\"Your text to analyze here\", \"Another example text\", \"I hate you\"]})\n",
    "check_toxicity(data, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                final_phrase  toxicity severe_toxicity    insult profanity  \\\n",
      "0  Your text to analyze here  0.016462        0.001044  0.008976  0.013644   \n",
      "1       Another example text  0.009676        0.000782  0.007475  0.012175   \n",
      "2                 I hate you  0.682712         0.03949  0.437884  0.335177   \n",
      "\n",
      "     threat identity_attack  \n",
      "0  0.007405        0.003589  \n",
      "1  0.006745        0.002442  \n",
      "2  0.043956        0.288834  \n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
